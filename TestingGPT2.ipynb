{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import transformers\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from infilling_gpt2 import get_model\r\n",
    "from tokenizer_util import get_tokenizer, tokenize"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rabir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model = get_model()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "tokenizer = get_tokenizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "s = 'She is a _ and a director'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "s = s.replace('_', '<|infillphrase|>')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "s"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'She is a <|infillphrase|> and a director'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "s = s + '<|startofinfill|>'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "input_ids = torch.tensor(tokenizer(s)['input_ids']).unsqueeze(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "input_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 3347,   318,   257, 50259,   392,   257,  3437, 50257]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model_path = os.path.join(os.path.expanduser('E:/ResearchWork/CurrentResearch/gpt2-ilm/ModelState'), 'GPT2FineTuned.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model.load_state_dict(torch.load(model_path, map_location = torch.device('cpu')))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def top_k_logits(logits, k, temp):\r\n",
    "    if k == 0:\r\n",
    "        return logits\r\n",
    "    if temp == 0:\r\n",
    "        return torch.argmax(logits, dim=-1).unsqueeze(-1)\r\n",
    "    if temp != 0:\r\n",
    "        logits = logits / temp\r\n",
    "    values, _ = torch.topk(logits, k)\r\n",
    "    min_values = values[:, -1]\r\n",
    "    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def top_p_logits(logits, top_p, filter_value = -float('Inf')):\r\n",
    "    if top_p == 0:\r\n",
    "        return logits\r\n",
    "    \r\n",
    "    #samp_probs = F.softmax(logits, dim = -1)\r\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending = True)\r\n",
    "    cumulative_probs = torch.cumsum(sorted_logits, dim = -1)\r\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\r\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\r\n",
    "    sorted_indices_to_remove[..., 0] = 0\r\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove) # Need to research this\r\n",
    "    logits[indices_to_remove] = filter_value\r\n",
    "    return logits"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def get_tokens(model, input_ids):\r\n",
    "    model.eval()\r\n",
    "    with torch.no_grad():\r\n",
    "        out = model(input_ids)\r\n",
    "        logits = out['logits']\r\n",
    "        print(logits.size())\r\n",
    "        logits = logits[:, -1, :]\r\n",
    "        #logits = top_k_logits(logits, k = 10, temp = 0.8)\r\n",
    "        logits = top_p_logits(logits, top_p = 0.95)\r\n",
    "        log_probs = F.softmax(logits, dim=-1)\r\n",
    "        output = torch.multinomial(log_probs, num_samples = 1)\r\n",
    "        input_ids = torch.cat((input_ids, output), axis = 1)\r\n",
    "        return input_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "t = 0\r\n",
    "while (t < 2):\r\n",
    "    input_ids = get_tokens(model, input_ids)\r\n",
    "    t += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 8, 50262])\n",
      "torch.Size([1, 9, 50262])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "input_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 3347,   318,   257, 50259,   392,   257,  3437, 50257,  2646,  9920]])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "tokenizer.decode(x for x in input_ids.squeeze(0))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'She is a <|infillphrase|> and a director <|startofinfill|>  film producer'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f13e882c2e3258ce20b832a92b4120b3253ef305fd3fc3162b85efa3e757aae"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}